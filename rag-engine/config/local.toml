# RAG 引擎配置文件
# config/local.toml

[app]
name = "rag-engine"
version = "0.1.0"
environment = "development"
debug = true
work_dir = "./data"
temp_dir = "/tmp"

[database]
[database.postgres]
url = "postgres://rag_user:rag_password@localhost:5432/rag_development"
max_connections = 20
min_connections = 5
connect_timeout = 30
idle_timeout = 600
max_lifetime = 3600

[database.vector]
provider = "qdrant"

[database.vector.qdrant]
url = "http://localhost:6333"
timeout = 30
max_retries = 3

[cache]
[cache.redis]
url = "redis://localhost:6379"
pool_size = 20
min_idle = 5
connect_timeout = 10
command_timeout = 5
max_retries = 3

[cache.memory]
max_size = 1073741824  # 1GB
default_ttl = 3600
cleanup_interval = 300

[network]
[network.http]
enabled = true
bind_address = "0.0.0.0:8080"
request_timeout = 30
max_body_size = 16777216  # 16MB

[network.http.cors]
allowed_origins = ["*"]
allowed_methods = ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
allowed_headers = ["*"]
allow_credentials = true

[network.http.rate_limit]
enabled = true
requests_per_second = 100
burst_size = 10

[network.grpc]
enabled = true
bind_address = "0.0.0.0:9090"
max_receive_message_size = 16777216
max_send_message_size = 16777216
connect_timeout = 30

[network.websocket]
enabled = true
bind_address = "0.0.0.0:8081"
max_connections = 1000
heartbeat_interval = 30
message_buffer_size = 1024

[embedding]
default_provider = "local"

[embedding.providers]
[embedding.providers.local]
model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_path = "./models/embedding"
device = "cpu"
batch_size = 32
max_length = 512

[embedding.providers.openai]
api_key = ""
model = "text-embedding-ada-002"
timeout = 30
max_retries = 3

[embedding.providers.huggingface]
api_key = ""
model = "sentence-transformers/all-MiniLM-L6-v2"
timeout = 30

[embedding.batch]
size = 32
timeout_ms = 100
max_queue_size = 1000

[embedding.cache]
enabled = true
ttl = 3600
max_entries = 10000

[retrieval]
default_strategy = "hybrid"
default_top_k = 10

[retrieval.strategies]
[retrieval.strategies.dense]
similarity_threshold = 0.7

[retrieval.strategies.dense.search_params]
ef = 128
exact = false

[retrieval.strategies.sparse]
[retrieval.strategies.sparse.bm25]
k1 = 1.2
b = 0.75

[retrieval.strategies.hybrid]
dense_weight = 0.7
sparse_weight = 0.3

[retrieval.reranking]
enabled = false
model = "cross-encoder/ms-marco-MiniLM-L-6-v2"
top_k = 100

[retrieval.fusion]
method = "rrf"
rrf_k = 60.0

[llm]
default_provider = "openai"

[llm.providers]
[llm.providers.openai]
api_key = ""
model = "gpt-3.5-turbo"
timeout = 60

[llm.providers.anthropic]
api_key = ""
model = "claude-3-sonnet-20240229"
timeout = 60

[llm.providers.local]
model_path = "./models/llm"
device = "cpu"
max_context_length = 4096

[llm.conversation]
max_history_length = 10
context_window_size = 4096
memory_strategy = "sliding_window"

[llm.generation]
default_temperature = 0.7
max_tokens = 1000
streaming = true
timeout = 60

[concurrency]
worker_threads = 4

[concurrency.task_queue]
high_priority_capacity = 1000
medium_priority_capacity = 5000
low_priority_capacity = 10000

[concurrency.semaphores]
embedding_concurrency = 10
retrieval_concurrency = 20
llm_concurrency = 5

[observability]
[observability.logging]
level = "info"
format = "json"
output = "stdout"

[observability.metrics]
enabled = true
prometheus_address = "0.0.0.0:9091"
collection_interval = 60

[observability.tracing]
enabled = true
jaeger_endpoint = "http://localhost:14268/api/traces"
sample_rate = 0.1

[observability.health]
check_interval = 30
timeout = 10

[plugins]
plugin_dir = "./plugins"
enable_wasm = true

[plugins.wasm_runtime]
memory_limit = 67108864  # 64MB
execution_timeout = 30
max_instances = 100