//! # RAG æ ¸å¿ƒæœåŠ¡
//!
//! RAG ç³»ç»Ÿçš„ä¸»è¦ä¸šåŠ¡é€»è¾‘å®ç°
//! æ–‡ä»¶è·¯å¾„: rag-engine/src/services/rag_service.rs

use std::collections::HashMap;
use std::sync::Arc;
use uuid::Uuid;

use crate::cache::Cache;
use crate::config::RagConfig;
use crate::embedding::{EmbeddingService, EmbeddingConfig, EmbeddingProvider, LocalEmbeddingService};
use crate::error::{RagError, RagResult};
use crate::llm::{LLMService, LLMConfig, LLMProvider, LLMServiceFactory};
use crate::pipeline::{RAGPipeline, RAGPipelineBuilder, RAGPipelineConfig, MainRAGPipeline, PipelineResult};
use crate::retrieval::{RetrievalService, MainRetrievalService, RetrievalConfig, vector_store::{VectorStore, VectorStoreFactory, VectorStoreConfig, VectorStoreProvider}};
use crate::retrieval::InMemoryKeywordIndex;
use crate::types::*;

/// RAG æœåŠ¡çš„ä¸»è¦å®ç°
pub struct RagService {
    config: Arc<RagConfig>,
    pipeline: Arc<dyn RAGPipeline>,
    cache: Arc<dyn Cache>,
}

impl RagService {
    /// åˆ›å»ºæ–°çš„ RAG æœåŠ¡å®ä¾‹
    pub async fn new(config: RagConfig) -> RagResult<Self> {
        let config = Arc::new(config);

        // åˆå§‹åŒ–ç¼“å­˜
        let cache = Self::create_cache_service(&config).await?;

        // åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        let embedding_service = Self::create_embedding_service(&config).await?;
        let vector_store = Self::create_vector_store(&config).await?;
        let llm_service = Self::create_llm_service(&config).await?;
        let retrieval_service = Self::create_retrieval_service(
            &config,
            embedding_service.clone(),
            vector_store.clone(),
        ).await?;

        // æ„å»º RAG ç®¡é“
        let pipeline_config = RAGPipelineConfig {
            chunking: crate::pipeline::document_processor::ChunkingConfig {
                strategy: crate::pipeline::document_processor::ChunkingStrategy::Hybrid,
                max_chunk_size: config.processing.chunk_size,
                chunk_overlap: config.processing.chunk_overlap,
                min_chunk_size: 100,
                respect_sentence_boundaries: true,
                respect_paragraph_boundaries: true,
                preserve_structure: true,
            },
            embedding_batch_size: config.embedding.batch_size,
            vector_store_batch_size: 100,
            enable_preprocessing: true,
            enable_postprocessing: true,
            preprocessing_steps: vec![
                crate::pipeline::PreprocessingStep::TextCleaning,
                crate::pipeline::PreprocessingStep::LanguageDetection,
                crate::pipeline::PreprocessingStep::QualityFiltering,
            ],
            postprocessing_steps: vec![
                crate::pipeline::PostprocessingStep::DuplicateRemoval,
                crate::pipeline::PostprocessingStep::QualityScoring,
                crate::pipeline::PostprocessingStep::MetadataEnrichment,
            ],
            quality_threshold: 0.3,
            enable_parallel_processing: config.processing.enable_gpu,
            max_concurrent_jobs: config.concurrency.max_workers as u32,
        };

        let pipeline = RAGPipelineBuilder::new()
            .with_embedding_service(embedding_service)
            .with_vector_store(vector_store)
            .with_retrieval_service(retrieval_service)
            .with_llm_service(llm_service)
            .with_config(pipeline_config)
            .build()?;

        Ok(Self {
            config,
            pipeline: Arc::new(pipeline),
            cache,
        })
    }

    /// åˆ›å»ºç¼“å­˜æœåŠ¡
    async fn create_cache_service(config: &RagConfig) -> RagResult<Arc<dyn Cache>> {
        match config.cache.backend.as_str() {
            "redis" => {
                // TODO: å®ç° Redis ç¼“å­˜
                Err(RagError::ConfigurationError("Redis ç¼“å­˜å°šæœªå®ç°".to_string()))
            }
            "memory" => {
                let cache = crate::cache::MemoryCache::new();
                Ok(Arc::new(cache))
            }
            _ => {
                let cache = crate::cache::MemoryCache::new();
                Ok(Arc::new(cache))
            }
        }
    }

    /// åˆ›å»ºåµŒå…¥æœåŠ¡
    async fn create_embedding_service(config: &RagConfig) -> RagResult<Arc<dyn EmbeddingService>> {
        let embedding_config = EmbeddingConfig {
            model_name: config.embedding.model.clone(),
            provider: match config.embedding.provider.as_str() {
                "openai" => EmbeddingProvider::OpenAI,
                "huggingface" => EmbeddingProvider::HuggingFace,
                "cohere" => EmbeddingProvider::Cohere,
                "local" => EmbeddingProvider::Local,
                _ => EmbeddingProvider::Local,
            },
            dimensions: config.embedding.dimensions,
            max_tokens: 8192,
            batch_size: config.embedding.batch_size,
            api_key: std::env::var("EMBEDDING_API_KEY").ok(),
            base_url: None,
            model_params: HashMap::new(),
        };

        match embedding_config.provider {
            EmbeddingProvider::OpenAI => {
                let service = crate::embedding::OpenAIEmbeddingService::new(embedding_config)?;
                Ok(Arc::new(service))
            }
            EmbeddingProvider::HuggingFace => {
                let service = crate::embedding::HuggingFaceEmbeddingService::new(embedding_config)?;
                Ok(Arc::new(service))
            }
            EmbeddingProvider::Local => {
                let service = LocalEmbeddingService::new(embedding_config)?;
                Ok(Arc::new(service))
            }
            _ => {
                let service = LocalEmbeddingService::new(embedding_config)?;
                Ok(Arc::new(service))
            }
        }
    }

    /// åˆ›å»ºå‘é‡å­˜å‚¨
    async fn create_vector_store(config: &RagConfig) -> RagResult<Arc<dyn VectorStore>> {
        let vector_config = VectorStoreConfig {
            provider: match config.vector_db.provider.as_str() {
                "qdrant" => VectorStoreProvider::Qdrant,
                "pinecone" => VectorStoreProvider::Pinecone,
                "weaviate" => VectorStoreProvider::Weaviate,
                "memory" => VectorStoreProvider::InMemory,
                _ => VectorStoreProvider::InMemory,
            },
            connection_string: config.vector_db.url.clone(),
            collection_name: config.vector_db.collection.clone(),
            dimensions: config.embedding.dimensions,
            distance_metric: crate::retrieval::vector_store::DistanceMetric::Cosine,
            index_config: crate::retrieval::vector_store::IndexConfig {
                index_type: crate::retrieval::vector_store::IndexType::HNSW,
                ef_construct: Some(200),
                m: Some(16),
                quantization: None,
            },
            batch_size: 100,
            timeout_seconds: 30,
        };

        let store = VectorStoreFactory::create_store(vector_config).await?;
        Ok(store)
    }

    /// åˆ›å»º LLM æœåŠ¡
    async fn create_llm_service(config: &RagConfig) -> RagResult<Arc<dyn LLMService>> {
        let llm_config = LLMConfig {
            provider: match config.llm.provider.as_str() {
                "openai" => LLMProvider::OpenAI,
                "anthropic" => LLMProvider::Anthropic,
                "google" => LLMProvider::Google,
                "local" => LLMProvider::Local,
                _ => LLMProvider::OpenAI,
            },
            model_name: config.llm.model.clone(),
            api_key: std::env::var("LLM_API_KEY").ok(),
            base_url: None,
            max_tokens: config.llm.max_tokens,
            temperature: config.llm.temperature,
            top_p: 0.9,
            presence_penalty: 0.0,
            frequency_penalty: 0.0,
            timeout_seconds: 60,
            system_prompt: config.llm.system_prompt.clone(),
        };

        let service = LLMServiceFactory::create_service(llm_config)?;
        Ok(service)
    }

    /// åˆ›å»ºæ£€ç´¢æœåŠ¡
    async fn create_retrieval_service(
        config: &RagConfig,
        embedding_service: Arc<dyn EmbeddingService>,
        vector_store: Arc<dyn VectorStore>,
    ) -> RagResult<Arc<dyn RetrievalService>> {
        let retrieval_config = RetrievalConfig {
            strategy: crate::retrieval::RetrievalStrategy::Hybrid,
            vector_weight: 0.7,
            keyword_weight: 0.3,
            semantic_weight: 0.0,
            enable_reranking: true,
            rerank_model: Some("cross-encoder".to_string()),
            max_candidates: 100,
            similarity_threshold: 0.3,
            diversity_threshold: 0.8,
            enable_query_expansion: false,
            enable_filter_optimization: true,
        };

        let keyword_index = Arc::new(InMemoryKeywordIndex::new());
        let reranker = Some(Box::new(crate::retrieval::CrossEncoderReranker::new(
            "cross-encoder/ms-marco-MiniLM-L-6-v2".to_string()
        )) as Box<dyn crate::retrieval::Reranker>);

        let service = MainRetrievalService::new(
            vector_store,
            embedding_service,
            keyword_index,
            reranker,
            retrieval_config,
        );

        Ok(Arc::new(service))
    }

    /// å¤„ç†æ–‡æ¡£ä¸Šä¼ 
    pub async fn upload_document(
        &self,
        data: Vec<u8>,
        filename: String,
        metadata: Option<HashMap<String, String>>,
    ) -> RagResult<PipelineResult> {
        tracing::info!("ä¸Šä¼ æ–‡æ¡£: {}", filename);

        // æ£€æŸ¥ç¼“å­˜
        let cache_key = format!("doc_upload:{}:{}", filename,
                                self.calculate_content_hash(&data));

        if let Ok(Some(cached_result)) = self.cache.get::<PipelineResult>(&cache_key).await {
            tracing::debug!("è¿”å›ç¼“å­˜çš„å¤„ç†ç»“æœ");
            return Ok(cached_result);
        }

        // å¤„ç†æ–‡æ¡£
        let result = self.pipeline.process_document(data, filename, metadata).await?;

        // ç¼“å­˜ç»“æœ
        if result.success {
            let _ = self.cache.set(&cache_key, &result, 3600).await; // ç¼“å­˜1å°æ—¶
        }

        Ok(result)
    }

    /// æ‰¹é‡ä¸Šä¼ æ–‡æ¡£
    pub async fn upload_documents(
        &self,
        documents: Vec<(Vec<u8>, String, Option<HashMap<String, String>>)>,
    ) -> RagResult<Vec<PipelineResult>> {
        tracing::info!("æ‰¹é‡ä¸Šä¼  {} ä¸ªæ–‡æ¡£", documents.len());

        let results = self.pipeline.process_documents(documents).await?;

        tracing::info!("æ‰¹é‡ä¸Šä¼ å®Œæˆ: {} ä¸ªæˆåŠŸ, {} ä¸ªå¤±è´¥",
            results.iter().filter(|r| r.success).count(),
            results.iter().filter(|r| !r.success).count()
        );

        Ok(results)
    }

    /// åˆ é™¤æ–‡æ¡£
    pub async fn delete_document(&self, document_id: Uuid) -> RagResult<()> {
        tracing::info!("åˆ é™¤æ–‡æ¡£: {}", document_id);

        self.pipeline.delete_document(document_id).await?;

        // æ¸…ç†ç›¸å…³ç¼“å­˜
        let cache_pattern = format!("doc:{}:*", document_id);
        // æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å®ç°æ¨¡å¼åŒ¹é…åˆ é™¤ï¼Œç®€åŒ–ç‰ˆæœ¬ä¸æ”¯æŒ

        Ok(())
    }

    /// é‡æ–°ç´¢å¼•æ–‡æ¡£
    pub async fn reindex_document(&self, document_id: Uuid) -> RagResult<PipelineResult> {
        tracing::info!("é‡æ–°ç´¢å¼•æ–‡æ¡£: {}", document_id);
        self.pipeline.reindex_document(document_id).await
    }

    /// æ‰§è¡Œæœç´¢
    pub async fn search(&self, query: Query) -> RagResult<SearchResult> {
        tracing::debug!("æ‰§è¡Œæœç´¢: {}", query.text);

        // æ£€æŸ¥ç¼“å­˜
        let cache_key = format!("search:{}:{}:{}",
                                query.text, query.options.strategy, query.options.top_k);

        if let Ok(Some(cached_result)) = self.cache.get::<SearchResult>(&cache_key).await {
            tracing::debug!("è¿”å›ç¼“å­˜çš„æœç´¢ç»“æœ");
            return Ok(cached_result);
        }

        // æ‰§è¡Œæœç´¢
        let result = self.pipeline.query(query).await?;

        // ç¼“å­˜ç»“æœ
        let _ = self.cache.set(&cache_key, &result, 300).await; // ç¼“å­˜5åˆ†é’Ÿ

        Ok(result)
    }

    /// ç”Ÿæˆ RAG å›ç­”
    pub async fn generate_answer(
        &self,
        question: &str,
        conversation_history: Option<Vec<crate::llm::ChatMessage>>,
        options: Option<QueryOptions>,
    ) -> RagResult<crate::llm::ChatResponse> {
        tracing::info!("ç”Ÿæˆå›ç­”: {}", question);

        // æ£€æŸ¥ç¼“å­˜
        let cache_key = format!("answer:{}:{}",
                                question,
                                conversation_history.as_ref().map_or(0, |h| h.len())
        );

        if let Ok(Some(cached_response)) = self.cache.get::<crate::llm::ChatResponse>(&cache_key).await {
            tracing::debug!("è¿”å›ç¼“å­˜çš„å›ç­”");
            return Ok(cached_response);
        }

        // ç”Ÿæˆå›ç­”
        let response = self.pipeline.generate_response(question, conversation_history, options).await?;

        // ç¼“å­˜ç»“æœ
        let _ = self.cache.set(&cache_key, &response, 1800).await; // ç¼“å­˜30åˆ†é’Ÿ

        Ok(response)
    }

    /// æµå¼ç”Ÿæˆ RAG å›ç­”
    pub async fn generate_answer_stream(
        &self,
        question: &str,
        conversation_history: Option<Vec<crate::llm::ChatMessage>>,
        options: Option<QueryOptions>,
    ) -> RagResult<Box<dyn tokio_stream::Stream<Item = RagResult<crate::llm::StreamChunk>> + Send + Unpin>> {
        tracing::info!("æµå¼ç”Ÿæˆå›ç­”: {}", question);

        // æµå¼å“åº”ä¸ä½¿ç”¨ç¼“å­˜
        self.pipeline.generate_response_stream(question, conversation_history, options).await
    }

    /// è·å–ç›¸ä¼¼æ–‡æ¡£
    pub async fn find_similar_documents(
        &self,
        document_id: Uuid,
        top_k: u32,
    ) -> RagResult<Vec<SearchResultItem>> {
        tracing::debug!("æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£: {}", document_id);

        // æ£€æŸ¥ç¼“å­˜
        let cache_key = format!("similar:{}:{}", document_id, top_k);

        if let Ok(Some(cached_results)) = self.cache.get::<Vec<SearchResultItem>>(&cache_key).await {
            tracing::debug!("è¿”å›ç¼“å­˜çš„ç›¸ä¼¼æ–‡æ¡£");
            return Ok(cached_results);
        }

        // æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£
        let results = self.pipeline.retrieval_service.find_similar(document_id, top_k).await?;

        // ç¼“å­˜ç»“æœ
        let _ = self.cache.set(&cache_key, &results, 600).await; // ç¼“å­˜10åˆ†é’Ÿ

        Ok(results)
    }

    /// ç”ŸæˆæŸ¥è¯¢å»ºè®®
    pub async fn suggest_queries(&self, partial_query: &str, limit: u32) -> RagResult<Vec<String>> {
        tracing::debug!("ç”ŸæˆæŸ¥è¯¢å»ºè®®: {}", partial_query);

        // æ£€æŸ¥ç¼“å­˜
        let cache_key = format!("suggest:{}:{}", partial_query, limit);

        if let Ok(Some(cached_suggestions)) = self.cache.get::<Vec<String>>(&cache_key).await {
            tracing::debug!("è¿”å›ç¼“å­˜çš„æŸ¥è¯¢å»ºè®®");
            return Ok(cached_suggestions);
        }

        // ç”Ÿæˆå»ºè®®
        let suggestions = self.pipeline.retrieval_service.suggest_queries(partial_query, limit).await?;

        // ç¼“å­˜ç»“æœ
        let _ = self.cache.set(&cache_key, &suggestions, 1800).await; // ç¼“å­˜30åˆ†é’Ÿ

        Ok(suggestions)
    }

    /// è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
    pub async fn get_stats(&self) -> RagResult<crate::pipeline::PipelineStats> {
        tracing::debug!("è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯");

        // æ£€æŸ¥ç¼“å­˜
        let cache_key = "system_stats";

        if let Ok(Some(cached_stats)) = self.cache.get::<crate::pipeline::PipelineStats>(&cache_key).await {
            tracing::debug!("è¿”å›ç¼“å­˜çš„ç»Ÿè®¡ä¿¡æ¯");
            return Ok(cached_stats);
        }

        // è·å–ç»Ÿè®¡ä¿¡æ¯
        let stats = self.pipeline.get_stats().await?;

        // ç¼“å­˜ç»“æœ
        let _ = self.cache.set(&cache_key, &stats, 60).await; // ç¼“å­˜1åˆ†é’Ÿ

        Ok(stats)
    }

    /// å¥åº·æ£€æŸ¥
    pub async fn health_check(&self) -> RagResult<HealthStatus> {
        tracing::debug!("æ‰§è¡Œå¥åº·æ£€æŸ¥");

        let mut status = HealthStatus {
            overall: "healthy".to_string(),
            components: HashMap::new(),
            timestamp: chrono::Utc::now(),
        };

        // æ£€æŸ¥ç®¡é“å¥åº·çŠ¶æ€
        match self.pipeline.health_check().await {
            Ok(_) => {
                status.components.insert("pipeline".to_string(), ComponentHealth {
                    status: "healthy".to_string(),
                    message: "æ‰€æœ‰ç»„ä»¶æ­£å¸¸".to_string(),
                    last_check: chrono::Utc::now(),
                });
            }
            Err(e) => {
                status.overall = "unhealthy".to_string();
                status.components.insert("pipeline".to_string(), ComponentHealth {
                    status: "unhealthy".to_string(),
                    message: e.to_string(),
                    last_check: chrono::Utc::now(),
                });
            }
        }

        // æ£€æŸ¥ç¼“å­˜å¥åº·çŠ¶æ€
        match self.cache.health_check().await {
            Ok(_) => {
                status.components.insert("cache".to_string(), ComponentHealth {
                    status: "healthy".to_string(),
                    message: "ç¼“å­˜æ­£å¸¸".to_string(),
                    last_check: chrono::Utc::now(),
                });
            }
            Err(e) => {
                status.components.insert("cache".to_string(), ComponentHealth {
                    status: "unhealthy".to_string(),
                    message: e.to_string(),
                    last_check: chrono::Utc::now(),
                });
            }
        }

        Ok(status)
    }

    /// æ¸…ç†ç¼“å­˜
    pub async fn clear_cache(&self) -> RagResult<()> {
        tracing::info!("æ¸…ç†æ‰€æœ‰ç¼“å­˜");

        self.cache.clear().await?;

        tracing::info!("ç¼“å­˜æ¸…ç†å®Œæˆ");
        Ok(())
    }

    /// è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
    pub async fn get_cache_stats(&self) -> RagResult<crate::cache::CacheStats> {
        self.cache.stats().await
    }

    /// è®¡ç®—å†…å®¹å“ˆå¸Œ
    fn calculate_content_hash(&self, data: &[u8]) -> String {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        data.hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }

    /// é¢„çƒ­ç³»ç»Ÿ
    pub async fn warmup(&self) -> RagResult<()> {
        tracing::info!("å¼€å§‹ç³»ç»Ÿé¢„çƒ­");

        // 1. æ£€æŸ¥æ‰€æœ‰ç»„ä»¶å¥åº·çŠ¶æ€
        self.health_check().await?;

        // 2. æ‰§è¡Œä¸€ä¸ªæµ‹è¯•æŸ¥è¯¢æ¥é¢„çƒ­æ¨¡å‹
        let test_query = Query {
            id: Uuid::new_v4(),
            text: "system warmup test".to_string(),
            options: QueryOptions {
                strategy: "hybrid".to_string(),
                top_k: 1,
                similarity_threshold: Some(0.0),
                filters: vec![],
                enable_reranking: false,
                rerank_top_k: None,
                workspace_id: None,
            },
            timestamp: chrono::Utc::now(),
        };

        let _ = self.search(test_query).await;

        tracing::info!("ç³»ç»Ÿé¢„çƒ­å®Œæˆ");
        Ok(())
    }

    /// ä¼˜é›…å…³é—­
    pub async fn shutdown(&self) -> RagResult<()> {
        tracing::info!("å¼€å§‹ä¼˜é›…å…³é—­ RAG æœåŠ¡");

        // æ¸…ç†ç¼“å­˜
        let _ = self.cache.cleanup().await;

        tracing::info!("RAG æœåŠ¡å…³é—­å®Œæˆ");
        Ok(())
    }
}

/// å¥åº·çŠ¶æ€
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct HealthStatus {
    pub overall: String,
    pub components: HashMap<String, ComponentHealth>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

/// ç»„ä»¶å¥åº·çŠ¶æ€
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ComponentHealth {
    pub status: String,
    pub message: String,
    pub last_check: chrono::DateTime<chrono::Utc>,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::{RagConfig, EmbeddingConfig, LLMConfig, VectorDBConfig, ProcessingConfig, CacheConfig, ConcurrencyConfig, NetworkConfig, ObservabilityConfig, SecurityConfig};

    fn create_test_config() -> RagConfig {
        RagConfig {
            embedding: EmbeddingConfig {
                provider: "local".to_string(),
                model: "test-model".to_string(),
                dimensions: 128,
                batch_size: 32,
            },
            llm: LLMConfig {
                provider: "openai".to_string(),
                model: "gpt-3.5-turbo".to_string(),
                max_tokens: 2000,
                temperature: 0.7,
                system_prompt: Some("You are a helpful assistant.".to_string()),
            },
            vector_db: VectorDBConfig {
                provider: "memory".to_string(),
                url: "memory://test".to_string(),
                collection: "test_collection".to_string(),
            },
            processing: ProcessingConfig {
                chunk_size: 1000,
                chunk_overlap: 200,
                enable_gpu: false,
            },
            cache: CacheConfig {
                backend: "memory".to_string(),
                ttl_seconds: 3600,
            },
            concurrency: ConcurrencyConfig {
                max_workers: 4,
                queue_size: 1000,
            },
            network: NetworkConfig {
                http: crate::config::HttpConfig {
                    bind_address: "127.0.0.1:8080".to_string(),
                    max_request_size: 10485760,
                    timeout_seconds: 30,
                },
                grpc: crate::config::GrpcConfig {
                    bind_address: "127.0.0.1:9090".to_string(),
                    max_message_size: 4194304,
                    timeout_seconds: 30,
                },
                websocket: crate::config::WebSocketConfig {
                    bind_address: "127.0.0.1:8081".to_string(),
                    max_connections: 1000,
                    heartbeat_interval: 30,
                },
            },
            observability: ObservabilityConfig {
                log_level: "info".to_string(),
                enable_metrics: true,
                enable_tracing: true,
                jaeger_endpoint: Some("http://localhost:14268/api/traces".to_string()),
            },
            security: SecurityConfig {
                enable_tls: false,
                cert_path: None,
                key_path: None,
                enable_auth: false,
                jwt_secret: None,
            },
        }
    }

    #[tokio::test]
    async fn test_rag_service_creation() {
        let config = create_test_config();
        let service = RagService::new(config).await;

        // æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•å¯èƒ½å¤±è´¥ï¼Œå› ä¸ºéœ€è¦å®é™…çš„æœåŠ¡ä¾èµ–
        // åœ¨å®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨ mock å¯¹è±¡
        match service {
            Ok(_) => {
                // æµ‹è¯•é€šè¿‡
            }
            Err(e) => {
                // é¢„æœŸçš„å¤±è´¥ï¼ˆå› ä¸ºç¼ºå°‘çœŸå®ä¾èµ–ï¼‰
                println!("Expected error: {}", e);
            }
        }
    }

    #[test]
    fn test_health_status() {
        let mut components = HashMap::new();
        components.insert("test".to_string(), ComponentHealth {
            status: "healthy".to_string(),
            message: "Test component".to_string(),
            last_check: chrono::Utc::now(),
        });

        let health_status = HealthStatus {
            overall: "healthy".to_string(),
            components,
            timestamp: chrono::Utc::now(),
        };

        assert_eq!(health_status.overall, "healthy");
        assert_eq!(health_status.components.len(), 1);
    }

    #[test]
    fn test_component_health() {
        let component = ComponentHealth {
            status: "healthy".to_string(),
            message: "Component is working".to_string(),
            last_check: chrono::Utc::now(),
        };

        assert_eq!(component.status, "healthy");
        assert!(!component.message.is_empty());
    }

    #[test]
    fn test_content_hash_calculation() {
        // åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ RagService æ¥æµ‹è¯•å“ˆå¸Œè®¡ç®—
        // æ³¨æ„ï¼šå®é™…æµ‹è¯•ä¸­éœ€è¦å®Œæ•´çš„æœåŠ¡å®ä¾‹

        let data1 = b"Hello, world!";
        let data2 = b"Hello, world!";
        let data3 = b"Different content";

        // æ¨¡æ‹Ÿå“ˆå¸Œè®¡ç®—é€»è¾‘
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher1 = DefaultHasher::new();
        data1.hash(&mut hasher1);
        let hash1 = format!("{:x}", hasher1.finish());

        let mut hasher2 = DefaultHasher::new();
        data2.hash(&mut hasher2);
        let hash2 = format!("{:x}", hasher2.finish());

        let mut hasher3 = DefaultHasher::new();
        data3.hash(&mut hasher3);
        let hash3 = format!("{:x}", hasher3.finish());

        assert_eq!(hash1, hash2); // ç›¸åŒå†…å®¹åº”è¯¥æœ‰ç›¸åŒå“ˆå¸Œ
        assert_ne!(hash1, hash3); // ä¸åŒå†…å®¹åº”è¯¥æœ‰ä¸åŒå“ˆå¸Œ
    }

    #[tokio::test]
    async fn test_cache_key_generation() {
        // æµ‹è¯•ç¼“å­˜é”®ç”Ÿæˆé€»è¾‘
        let filename = "test.txt";
        let content_hash = "abcdef123456";
        let cache_key = format!("doc_upload:{}:{}", filename, content_hash);

        assert_eq!(cache_key, "doc_upload:test.txt:abcdef123456");

        let search_key = format!("search:{}:{}:{}", "test query", "hybrid", 10);
        assert_eq!(search_key, "search:test query:hybrid:10");
    }

    // æ³¨æ„ï¼šæ›´å¤šçš„é›†æˆæµ‹è¯•éœ€è¦åœ¨æœ‰å®Œæ•´ä¾èµ–çš„ç¯å¢ƒä¸­è¿è¡Œ
    // è¿™äº›æµ‹è¯•é€šå¸¸æ”¾åœ¨ tests/ ç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯å•å…ƒæµ‹è¯•ä¸­
}//! # RAG æœåŠ¡æ ¸å¿ƒå®ç°
//!
//! åè°ƒå„ä¸ªå­ç³»ç»Ÿï¼Œæä¾›å®Œæ•´çš„ RAG åŠŸèƒ½

use std::{collections::HashMap, sync::Arc};
use tokio::sync::RwLock;
use tracing::{info, error, warn, debug};
use uuid::Uuid;
use chrono::Utc;

use crate::{
    config::RagConfig,
    error::{RagError, RagResult},
    types::*,
    cache::Cache,
    pipeline::{DocumentProcessor, ProcessingPipeline},
    embedding::{EmbeddingService, EmbeddingProvider},
    retrieval::{RetrievalService, VectorStore},
    llm::{LlmService, LlmProvider},
};

/// RAG æœåŠ¡ä¸»ç»“æ„
pub struct RagService {
    /// é…ç½®
    config: Arc<RagConfig>,

    /// æ–‡æ¡£å¤„ç†ç®¡é“
    document_processor: Arc<DocumentProcessor>,

    /// åµŒå…¥æœåŠ¡
    embedding_service: Arc<EmbeddingService>,

    /// æ£€ç´¢æœåŠ¡
    retrieval_service: Arc<RetrievalService>,

    /// LLM æœåŠ¡
    llm_service: Arc<LlmService>,

    /// ç¼“å­˜æœåŠ¡
    cache: Arc<dyn Cache + Send + Sync>,

    /// å†…å­˜ä¸­çš„æ–‡æ¡£å­˜å‚¨ (ç®€åŒ–å®ç°)
    documents: Arc<RwLock<HashMap<Uuid, Document>>>,

    /// ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
    stats: Arc<RwLock<SystemStats>>,
}

/// ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Default)]
pub struct SystemStats {
    pub total_documents: u64,
    pub total_chunks: u64,
    pub total_queries: u64,
    pub total_embeddings_generated: u64,
    pub uptime_seconds: u64,
    pub memory_usage_bytes: u64,
}

impl RagService {
    /// åˆ›å»ºæ–°çš„ RAG æœåŠ¡å®ä¾‹
    pub async fn new(config: Arc<RagConfig>) -> RagResult<Self> {
        info!("ğŸ”§ åˆå§‹åŒ– RAG æœåŠ¡...");

        // åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        let document_processor = Arc::new(DocumentProcessor::new(config.clone()).await?);
        info!("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ");

        // åˆå§‹åŒ–åµŒå…¥æœåŠ¡
        let embedding_service = Arc::new(EmbeddingService::new(config.clone()).await?);
        info!("âœ… åµŒå…¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ");

        // åˆå§‹åŒ–æ£€ç´¢æœåŠ¡
        let retrieval_service = Arc::new(RetrievalService::new(config.clone()).await?);
        info!("âœ… æ£€ç´¢æœåŠ¡åˆå§‹åŒ–å®Œæˆ");

        // åˆå§‹åŒ– LLM æœåŠ¡
        let llm_service = Arc::new(LlmService::new(config.clone()).await?);
        info!("âœ… LLM æœåŠ¡åˆå§‹åŒ–å®Œæˆ");

        // åˆå§‹åŒ–ç¼“å­˜
        let cache = Arc::new(crate::cache::RedisCache::new(config.clone()).await?);
        info!("âœ… ç¼“å­˜æœåŠ¡åˆå§‹åŒ–å®Œæˆ");

        Ok(Self {
            config,
            document_processor,
            embedding_service,
            retrieval_service,
            llm_service,
            cache,
            documents: Arc::new(RwLock::new(HashMap::new())),
            stats: Arc::new(RwLock::new(SystemStats::default())),
        })
    }

    /// å¥åº·æ£€æŸ¥
    pub async fn health_check(&self) -> RagResult<()> {
        debug!("æ‰§è¡Œå¥åº·æ£€æŸ¥...");

        // æ£€æŸ¥å„ä¸ªæœåŠ¡çš„å¥åº·çŠ¶æ€
        self.embedding_service.health_check().await?;
        self.retrieval_service.health_check().await?;
        self.llm_service.health_check().await?;

        debug!("å¥åº·æ£€æŸ¥é€šè¿‡");
        Ok(())
    }

    /// æ£€æŸ¥æ•°æ®åº“è¿æ¥
    pub async fn check_database(&self) -> RagResult<()> {
        // ç®€åŒ–å®ç° - å®é™…åº”è¯¥æ£€æŸ¥ PostgreSQL è¿æ¥
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        Ok(())
    }

    /// æ£€æŸ¥å‘é‡æ•°æ®åº“è¿æ¥
    pub async fn check_vector_db(&self) -> RagResult<()> {
        self.retrieval_service.health_check().await
    }

    /// æ£€æŸ¥ç¼“å­˜è¿æ¥
    pub async fn check_cache(&self) -> RagResult<()> {
        self.cache.health_check().await
    }

    /// æ¸…ç†ç¼“å­˜
    pub async fn cleanup_cache(&self) -> RagResult<()> {
        self.cache.cleanup().await
    }

    /// è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
    pub async fn get_system_stats(&self) -> RagResult<serde_json::Value> {
        let stats = self.stats.read().await.clone();

        Ok(serde_json::json!({
            "documents": stats.total_documents,
            "chunks": stats.total_chunks,
            "queries": stats.total_queries,
            "embeddings": stats.total_embeddings_generated,
            "uptime_seconds": stats.uptime_seconds,
            "memory_usage_mb": stats.memory_usage_bytes / 1024 / 1024,
            "embedding_service": self.embedding_service.get_stats().await?,
            "retrieval_service": self.retrieval_service.get_stats().await?,
            "llm_service": self.llm_service.get_stats().await?
        }))
    }

    // ========================================================================
    // æ–‡æ¡£ç®¡ç†åŠŸèƒ½
    // ========================================================================

    /// ä¸Šä¼ æ–‡æ¡£
    pub async fn upload_document(
        &self,
        title: String,
        content: Vec<u8>,
        filename: String,
        workspace_id: Option<Uuid>,
        tags: Vec<String>,
        category: Option<String>,
    ) -> RagResult<Document> {
        info!("ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {}", title);

        // è§£ææ–‡æ¡£å†…å®¹
        let text_content = self.document_processor.extract_text(&content, &filename).await?;

        // åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        let mut document = Document::new(title.clone(), text_content);
        document.metadata.filename = Some(filename);
        document.metadata.tags = tags;
        document.metadata.category = category;
        document.metadata.file_size = Some(content.len() as u64);

        // æ£€æµ‹æ–‡æ¡£è¯­è¨€
        document.metadata.language = Some("zh".to_string()); // ç®€åŒ–å®ç°

        // æ–‡æ¡£åˆ†å—
        let chunks = self.document_processor.chunk_document(&document).await?;
        document.chunks = chunks;

        info!("ğŸ“„ æ–‡æ¡£åˆ†å—å®Œæˆ: {} ä¸ªå—", document.chunks.len());

        // ç”ŸæˆåµŒå…¥å‘é‡
        self.generate_embeddings_for_document(&mut document).await?;

        // å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        self.store_document_in_vector_db(&document).await?;

        // æ›´æ–°æ–‡æ¡£çŠ¶æ€
        document.status = DocumentStatus::Completed;
        document.updated_at = Utc::now();

        // ä¿å­˜åˆ°å†…å­˜å­˜å‚¨ (å®é™…åº”è¯¥ä¿å­˜åˆ°æ•°æ®åº“)
        {
            let mut docs = self.documents.write().await;
            docs.insert(document.id, document.clone());
        }

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        {
            let mut stats = self.stats.write().await;
            stats.total_documents += 1;
            stats.total_chunks += document.chunks.len() as u64;
        }

        info!("âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {}", document.id);
        Ok(document)
    }

    /// åˆ—å‡ºæ–‡æ¡£
    pub async fn list_documents(
        &self,
        page: u32,
        page_size: u32,
        workspace_id: Option<Uuid>,
    ) -> RagResult<Vec<Document>> {
        let docs = self.documents.read().await;
        let mut documents: Vec<Document> = docs
            .values()
            .filter(|doc| {
                workspace_id.map_or(true, |ws_id| {
                    // ç®€åŒ–å®ç° - å®é™…åº”è¯¥æ£€æŸ¥å·¥ä½œç©ºé—´æƒé™
                    true
                })
            })
            .cloned()
            .collect();

        // æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        documents.sort_by(|a, b| b.created_at.cmp(&a.created_at));

        // åˆ†é¡µ
        let start = ((page - 1) * page_size) as usize;
        let end = (start + page_size as usize).min(documents.len());

        Ok(documents[start..end].to_vec())
    }

    /// è·å–æ–‡æ¡£
    pub async fn get_document(&self, id: Uuid) -> RagResult<Option<Document>> {
        let docs = self.documents.read().await;
        Ok(docs.get(&id).cloned())
    }

    /// æ›´æ–°æ–‡æ¡£
    pub async fn update_document(
        &self,
        id: Uuid,
        updates: serde_json::Value,
    ) -> RagResult<Document> {
        let mut docs = self.documents.write().await;
        let document = docs.get_mut(&id).ok_or_else(|| {
            RagError::NotFoundError {
                resource_type: "document".to_string(),
                id: id.to_string(),
            }
        })?;

        // ç®€åŒ–çš„æ›´æ–°é€»è¾‘
        if let Some(title) = updates.get("title").and_then(|v| v.as_str()) {
            document.title = title.to_string();
        }

        if let Some(tags) = updates.get("tags").and_then(|v| v.as_array()) {
            document.metadata.tags = tags
                .iter()
                .filter_map(|v| v.as_str().map(|s| s.to_string()))
                .collect();
        }

        document.updated_at = Utc::now();
        document.version += 1;

        Ok(document.clone())
    }

    /// åˆ é™¤æ–‡æ¡£
    pub async fn delete_document(&self, id: Uuid) -> RagResult<()> {
        // ä»å‘é‡æ•°æ®åº“åˆ é™¤
        self.retrieval_service.delete_document(id).await?;

        // ä»å†…å­˜å­˜å‚¨åˆ é™¤
        let mut docs = self.documents.write().await;
        docs.remove(&id).ok_or_else(|| {
            RagError::NotFoundError {
                resource_type: "document".to_string(),
                id: id.to_string(),
            }
        })?;

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        {
            let mut stats = self.stats.write().await;
            stats.total_documents = stats.total_documents.saturating_sub(1);
        }

        info!("ğŸ—‘ï¸ æ–‡æ¡£å·²åˆ é™¤: {}", id);
        Ok(())
    }

    /// è·å–æ–‡æ¡£å—
    pub async fn get_document_chunks(&self, doc_id: Uuid) -> RagResult<Vec<Chunk>> {
        let docs = self.documents.read().await;
        let document = docs.get(&doc_id).ok_or_else(|| {
            RagError::NotFoundError {
                resource_type: "document".to_string(),
                id: doc_id.to_string(),
            }
        })?;

        Ok(document.chunks.clone())
    }

    /// é‡æ–°ç´¢å¼•æ–‡æ¡£
    pub async fn reindex_document(&self, id: Uuid) -> RagResult<()> {
        let document = {
            let docs = self.documents.read().await;
            docs.get(&id).cloned().ok_or_else(|| {
                RagError::NotFoundError {
                    resource_type: "document".to_string(),
                    id: id.to_string(),
                }
            })?
        };

        // åˆ é™¤æ—§çš„å‘é‡
        self.retrieval_service.delete_document(id).await?;

        // é‡æ–°ç”ŸæˆåµŒå…¥å¹¶å­˜å‚¨
        let mut updated_doc = document.clone();
        self.generate_embeddings_for_document(&mut updated_doc).await?;
        self.store_document_in_vector_db(&updated_doc).await?;

        // æ›´æ–°æ–‡æ¡£
        {
            let mut docs = self.documents.write().await;
            docs.insert(id, updated_doc);
        }

        info!("ğŸ”„ æ–‡æ¡£é‡æ–°ç´¢å¼•å®Œæˆ: {}", id);
        Ok(())
    }

    // ========================================================================
    // æ£€ç´¢åŠŸèƒ½
    // ========================================================================

    /// æ‰§è¡Œæœç´¢
    pub async fn search(&self, query: Query) -> RagResult<SearchResult> {
        info!("ğŸ” æ‰§è¡Œæœç´¢: {}", query.text);
        let start_time = std::time::Instant::now();

        // æ£€æŸ¥ç¼“å­˜
        let cache_key = format!("search:{}:{}",
                                blake3::hash(query.text.as_bytes()).to_hex(),
                                blake3::hash(serde_json::to_string(&query.options)?.as_bytes()).to_hex()
        );

        if let Ok(Some(cached_result)) = self.cache.get::<SearchResult>(&cache_key).await {
            debug!("ğŸ¯ è¿”å›ç¼“å­˜çš„æœç´¢ç»“æœ");
            return Ok(cached_result);
        }

        // ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        let query_embedding = self.embedding_service
            .generate_embedding(&query.text)
            .await?;

        // æ‰§è¡Œæ£€ç´¢
        let retrieval_start = std::time::Instant::now();
        let search_results = self.retrieval_service.search(
            &query_embedding,
            &query.options,
        ).await?;
        let retrieval_time = retrieval_start.elapsed();

        // æ„å»ºæœç´¢ç»“æœ
        let total_time = start_time.elapsed();
        let search_result = SearchResult {
            query_id: query.id,
            results: search_results,
            metadata: SearchMetadata {
                total_time_ms: total_time.as_millis() as u64,
                retrieval_time_ms: retrieval_time.as_millis() as u64,
                reranking_time_ms: None,
                strategy_used: query.options.strategy.clone(),
                total_candidates: 0, // ç®€åŒ–å®ç°
                filtered_count: 0,
                returned_count: 0,
                index_stats: IndexStats {
                    total_documents: 0,
                    total_chunks: 0,
                    index_size_bytes: 0,
                    last_updated: Utc::now(),
                },
            },
            timestamp: Utc::now(),
        };

        // ç¼“å­˜ç»“æœ
        let _ = self.cache.set(&cache_key, &search_result, 300).await; // 5åˆ†é’Ÿç¼“å­˜

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        {
            let mut stats = self.stats.write().await;
            stats.total_queries += 1;
        }

        info!("âœ… æœç´¢å®Œæˆï¼Œè€—æ—¶: {}ms", total_time.as_millis());
        Ok(search_result)
    }

    // ========================================================================
    // å†…éƒ¨è¾…åŠ©æ–¹æ³•
    // ========================================================================

    /// ä¸ºæ–‡æ¡£ç”ŸæˆåµŒå…¥å‘é‡
    async fn generate_embeddings_for_document(&self, document: &mut Document) -> RagResult<()> {
        info!("ğŸ§  ä¸ºæ–‡æ¡£ç”ŸæˆåµŒå…¥å‘é‡: {}", document.id);

        for chunk in &mut document.chunks {
            if chunk.embedding.is_none() {
                let embedding = self.embedding_service
                    .generate_embedding(&chunk.content)
                    .await?;
                chunk.embedding = Some(embedding);
            }
        }

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        {
            let mut stats = self.stats.write().await;
            stats.total_embeddings_generated += document.chunks.len() as u64;
        }

        info!("âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆ: {} ä¸ª", document.chunks.len());
        Ok(())
    }

    /// å°†æ–‡æ¡£å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    async fn store_document_in_vector_db(&self, document: &Document) -> RagResult<()> {
        info!("ğŸ’¾ å­˜å‚¨æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“: {}", document.id);

        self.retrieval_service.store_document(document).await?;

        info!("âœ… æ–‡æ¡£å­˜å‚¨å®Œæˆ");
        Ok(())
    }
}

// ============================================================================
// å ä½å®ç° - è¿™äº›æ¨¡å—å°†åœ¨åç»­å®ç°
// ============================================================================

/// æ–‡æ¡£å¤„ç†å™¨å ä½å®ç°
pub struct DocumentProcessor {
    config: Arc<RagConfig>,
}

impl DocumentProcessor {
    pub async fn new(config: Arc<RagConfig>) -> RagResult<Self> {
        Ok(Self { config })
    }

    pub async fn extract_text(&self, content: &[u8], filename: &str) -> RagResult<String> {
        // ç®€åŒ–å®ç° - å‡è®¾éƒ½æ˜¯æ–‡æœ¬æ–‡ä»¶
        String::from_utf8(content.to_vec())
            .map_err(|e| RagError::PipelineError(
                crate::error::PipelineError::DocumentParsingFailed {
                    file: filename.to_string(),
                    error: e.to_string(),
                }
            ))
    }

    pub async fn chunk_document(&self, document: &Document) -> RagResult<Vec<Chunk>> {
        // ç®€åŒ–çš„åˆ†å—å®ç°
        let chunk_size = 512;
        let overlap = 50;
        let mut chunks = Vec::new();

        let words: Vec<&str> = document.content.split_whitespace().collect();
        let mut start = 0;

        while start < words.len() {
            let end = (start + chunk_size).min(words.len());
            let chunk_content = words[start..end].join(" ");

            let chunk = Chunk::new(
                document.id,
                chunk_content,
                ChunkPosition {
                    start_char: start * 5, // ç²—ç•¥ä¼°è®¡
                    end_char: end * 5,
                    page_number: None,
                    line_number: None,
                },
            );

            chunks.push(chunk);

            if end >= words.len() {
                break;
            }

            start = end - overlap;
        }

        Ok(chunks)
    }
}

/// åµŒå…¥æœåŠ¡å ä½å®ç°
pub struct EmbeddingService {
    config: Arc<RagConfig>,
}

impl EmbeddingService {
    pub async fn new(config: Arc<RagConfig>) -> RagResult<Self> {
        Ok(Self { config })
    }

    pub async fn health_check(&self) -> RagResult<()> {
        Ok(())
    }

    pub async fn get_stats(&self) -> RagResult<serde_json::Value> {
        Ok(serde_json::json!({
            "provider": self.config.embedding.default_provider,
            "cache_hits": 0,
            "cache_misses": 0
        }))
    }

    pub async fn generate_embedding(&self, text: &str) -> RagResult<EmbeddingVector> {
        // æ¨¡æ‹ŸåµŒå…¥å‘é‡ç”Ÿæˆ
        let mut embedding = vec![0.0f32; 384]; // å‡è®¾ 384 ç»´
        for (i, byte) in text.bytes().enumerate() {
            if i >= embedding.len() {
                break;
            }
            embedding[i] = (byte as f32 - 128.0) / 128.0; // ç®€å•çš„ä¼ªéšæœºåŒ–
        }

        // ç®€å•çš„å½’ä¸€åŒ–
        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm > 0.0 {
            for x in &mut embedding {
                *x /= norm;
            }
        }

        Ok(embedding)
    }
}

/// æ£€ç´¢æœåŠ¡å ä½å®ç°
pub struct RetrievalService {
    config: Arc<RagConfig>,
    // ç®€åŒ–çš„å†…å­˜å‘é‡å­˜å‚¨
    vector_store: Arc<RwLock<Vec<(Uuid, ChunkId, EmbeddingVector, String)>>>,
}

impl RetrievalService {
    pub async fn new(config: Arc<RagConfig>) -> RagResult<Self> {
        Ok(Self {
            config,
            vector_store: Arc::new(RwLock::new(Vec::new())),
        })
    }

    pub async fn health_check(&self) -> RagResult<()> {
        Ok(())
    }

    pub async fn get_stats(&self) -> RagResult<serde_json::Value> {
        let store = self.vector_store.read().await;
        Ok(serde_json::json!({
            "total_vectors": store.len(),
            "strategy": self.config.retrieval.default_strategy
        }))
    }

    pub async fn store_document(&self, document: &Document) -> RagResult<()> {
        let mut store = self.vector_store.write().await;

        for chunk in &document.chunks {
            if let Some(embedding) = &chunk.embedding {
                store.push((
                    document.id,
                    chunk.id,
                    embedding.clone(),
                    chunk.content.clone(),
                ));
            }
        }

        Ok(())
    }

    pub async fn delete_document(&self, doc_id: Uuid) -> RagResult<()> {
        let mut store = self.vector_store.write().await;
        store.retain(|(id, _, _, _)| *id != doc_id);
        Ok(())
    }

    pub async fn search(
        &self,
        query_embedding: &EmbeddingVector,
        options: &QueryOptions,
    ) -> RagResult<Vec<SearchResultItem>> {
        let store = self.vector_store.read().await;
        let mut results = Vec::new();

        // ç®€åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
        for (doc_id, chunk_id, embedding, content) in store.iter() {
            let similarity = cosine_similarity(query_embedding, embedding);

            if similarity >= options.similarity_threshold.unwrap_or(0.0) {
                // æ¨¡æ‹Ÿä»æ–‡æ¡£å­˜å‚¨è·å–å®Œæ•´å—ä¿¡æ¯
                let chunk = Chunk {
                    id: *chunk_id,
                    document_id: *doc_id,
                    content: content.clone(),
                    metadata: ChunkMetadata::from_content(content),
                    embedding: Some(embedding.clone()),
                    position: ChunkPosition {
                        start_char: 0,
                        end_char: content.len(),
                        page_number: None,
                        line_number: None,
                    },
                    created_at: Utc::now(),
                };

                results.push(SearchResultItem {
                    chunk,
                    score: similarity,
                    rank: 0, // å°†åœ¨æ’åºåè®¾ç½®
                    highlights: vec![content.clone()], // ç®€åŒ–å®ç°
                    explanation: None,
                });
            }
        }

        // æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap());

        // è®¾ç½®æ’åå¹¶é™åˆ¶ç»“æœæ•°é‡
        for (i, result) in results.iter_mut().enumerate() {
            result.rank = (i + 1) as u32;
        }

        results.truncate(options.top_k as usize);

        Ok(results)
    }
}

/// è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    if a.len() != b.len() {
        return 0.0;
    }

    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();

    if norm_a == 0.0 || norm_b == 0.0 {
        0.0
    } else {
        dot_product / (norm_a * norm_b)
    }
}

/// LLM æœåŠ¡å ä½å®ç°
pub struct LlmService {
    config: Arc<RagConfig>,
}

impl LlmService {
    pub async fn new(config: Arc<RagConfig>) -> RagResult<Self> {
        Ok(Self { config })
    }

    pub async fn health_check(&self) -> RagResult<()> {
        Ok(())
    }

    pub async fn get_stats(&self) -> RagResult<serde_json::Value> {
        Ok(serde_json::json!({
            "provider": self.config.llm.default_provider,
            "total_tokens": 0
        }))
    }
}